\section{Odległość edycyjna}

\begin{algorithm}[H]
    \caption{Odległość edycyjna}
    \Input{Słowa $t_1, t_2 \in \A^+$ ($|t_i| = n_i$ dla $i = 1, 2$)}
    \Output{Minimalna liczba operacji typu \emph{insert}, \emph{delete} i \emph{substitute} przeprowadzająca słowo $t_1$ w $t_2$.}
\end{algorithm}

Zwróćmy uwagę, że analogiczny problem, w którym dozwolona jest tylko operacja \emph{substitute} to obliczanie odległości Hamminga. Można ją wprost wyznaczyć przeglądając $t_1$ i $t_2$ od lewej do prawej, zliczając pozycje, dla których $t_1[i] \neq t_2[i]$.

\subsection{Algorytm Fischera-Wagnera}

Podstawowy algorytm obliczania odległości edycyjnej sprowadza się do sekwencyjnego obliczania wartości funkcji rekurencyjnej
\begin{align*}
  d_e(i, j) =
  \begin{cases}
    0 & \text{gdy $i = 0$ lub $j = 0$,} \\
    d_e(i - 1, j - 1) & \text{gdy $t_1[i] = t_2[j]$,} \\
    \min\{d_e(i - 1, j - 1), d_e(i - 1, j), d_e(i - 1, j - 1)\} + 1 & \text{gdy $t_1[i] \neq t_2[j]$.}
  \end{cases}
\end{align*}
Poprawność algorytmu wynika z indukcji ze względu na porządek par $(i, j)$. Czas działania algorytmu to $O(n_1 n_2)$.
Jak widać z powyższego wzoru, do obliczeń wystarczy zapamiętanie poprzedniego wiersza lub kolumny, więc wymagana pamięć wynosi $O(\min\{n_1, n_2\})$.

\begin{code}
\captionof{listing}{Obliczanie odległości edycyjnej w czasie $O(n_1 n_2)$ i pamięci $O(\min\{n_1, n_2\})$}
\inputminted{python}{code/other/edit-distance.py}
\label{alg:edit-distance}
\end{code}

Technika może być łatwo uogólniona do dowolnych macierzy ocen, przypisujących różne wagi za odległości dla operacji \emph{insert}, \emph{delete} i \emph{substitute}. Przykładem takiego wykorzystania są rodziny macierzy PAM i BLOSUM dla algorytmów dopasowań ciągów w zastosowaniach bioinformatycznych.

Można również łatwo zamienić problem na obliczanie funkcji podobieństwa jako maksymalnej (zamiast minimalnej) wartości dla danej macierzy operacji, tym razem interpretowanej jako macierz nagród \citep{sellers1974theory}.

\subsection{Technika czterech Rosjan}

Dla obliczeń macierzowych istnieje ogólna technika przyspieszania obliczeń, nazywana \emph{Four Russians speedup}\footnote{Najbardziej znanym z autorów był Jefim Dinic, znany również z algorytmu dla znajdowania maksymalnego przepływu w grafach.}.

Główna idea tej metody polega na podziale wejścia na małe bloki o niewielkim zakresie wartości.
Załóżmy, że pewien problem jest w ogólności rozwiązywany przez funkcję $y_n = f(y_{n - 1}, x_n)$, a jedna iteracja wymaga czasu $F(n)$ -- więc całość wykonuje się w czasie $\sum_{i = 1}^{n - 1} F(i)$.

Załóżmy teraz, że każda pozycja wejściowa pochodzi z małego zbioru $X$, a każda pozycja wyjściowa również z małego zbioru $Y$.
Wówczas dla dowolnego $k$ zawsze możemy obliczyć z wyprzedzeniem tablicę wszystkich $|X|^k |Y|$ wartości funkcji $f^{(k)}$, czyli $k$-krotnego złożenia $f$.
Wówczas można policzyć $f_n$ wykorzystując $\frac{n}{k}$ wyszukiwania w tablicy. Całkowity czas działania (zakładając stały dostęp do tablicy wartości) wynosi zatem
\begin{align*}
  |X|^k |Y| \sum_{i = 1}^{k - 1} F(i) + \frac{n}{k}.
\end{align*}
Przykładowo dla $F(n) = n^2$, $|X| = O(1)$ i $|Y| = O(n)$ wystarczy dobrać $k = \log_{|X|}{n}$, aby zmniejszyć czas wykonania z $O(n^3)$ do $O(n^2 \log^3{n})$.

Metoda ta używana jest m.in. do mnożenia i odwracania macierzy binarnych, a także do efektywnego liczenia domknięcia przechodniego grafu. Można ją również wykorzystać do optymalizacji algorytmu obliczającego odległość edycyjną \citep{masek1980faster}. Technika ta również uogólnia się na różnorodne macierze nagród i kar.

\paragraph{Omówienie algorytmu}

Odległość edycyjna pomiędzy dwoma słowami o długościach $n$ i $m$ jest definiowana jako minimalny koszt sekwencji operacji edytujących (dodania znaku, usunięcia znaku, zamiany jednego znaku w inny), które prowadzą do transformacji jednego słowa w drugie. Szczególnym przypadkiem problemu znajdywania odległości edycyjnej jest szukanie najdłuższego wspólnego podciągu dwóch słów. Rozwiązanie tego podproblemu w czasie szybszym niż $O(n^{2-\epsilon})$ (gdy słowa mają tę samą długość $n$), prowadziłoby do obalenia SETH. Autorzy pracy pokazują algorytm na szukanie odległości edycyjnej dwóch słów działający w czasie $O(n \cdot max(1,m/log n))$ pod warunkiem, że koszty operacji edycyjnych są całkowitymi wielokrotnościami tej samej liczby rzeczywistej, a alfabet jest skończony.

\paragraph{Przyjęte oznaczenia}

$A_n$ - n-ty znak słowa $A$\\
$A^{i,j}$ - słowo $A_i...A_j$\\
$D_a$ - koszt usunięcia znaku a\\
$I_a$ - koszt wstawienia znaku a\\
$R_{a,b}$ - koszt zamiany znaku a na znak b\\
$\delta$ - odległość edycyjna, minimum po sumie kosztów wszystkich możliwych sekwencji edycyjnych,
$\delta_{i,j}$ - odległość edycyjna między słowami $A^{1,i}$ oraz $B^{1,j}$\\

\paragraph{Algorytm Wagnera i Fischera}

Algorytm opisany w pracy opiera się na pomyśle z innego algorytmu, autorstwa Wagnera i Fischera. Algorytm ten wyznacza odległość edycyjną dynamicznie, w czasie proporcjonalnym do iloczynu długości słów. Tworzy w tym celu pomocniczą macierz o rozmiarze $(|A|+1) \times (|B|+1)$, w której komórka (i,j) odpowiada kosztowi $\delta_{i,j}$. Poniższe twierdzenia prezentują sposób obliczania bazy i kolejnych komórek:\\

\textbf{Twierdzenie 1}\\\\
$\delta_{0,0} = 0$ $\forall i,j : 1 \leq i \leq |A|, 1 \leq j \leq |B|$\\
$\delta_{i,0} = \sum_{1 \leq r \leq i}D_{A_r}$ oraz
$\delta_{0,j} = \sum_{1 \leq r \leq j}I_{B_r}$\\

\textbf{Twierdzenie 2}\\\\
$\delta_{i, j} = min(\delta_{i-1,j-1}+R_{A_i,B_j}, \delta_{i-1,j}+D_{A_i}, \delta_{i,j-1}+I_{B_j})$

\paragraph{Przyspieszenie metodą czterech Rosjan}

Technika "czterech Rosjan" pochodzi z algorytmu obliczania przechodniego domknięcia grafu skierowanego poprzez podział macierzy sąsiedztwa na podmacierze o niewielkiej ilości wierszy. Okazuje się, że podobny pomysł można wykorzystać do asymptotycznego przyspieszenia powyższego algorytmu, właśnie dzięki podziałowi obliczenia na dużo mniejszych obliczeń.\\

Zauważmy, że podmacierz $(i,j,m)$ macierzy edycyjnej z algorytmu Wagnera-Fischera - zaczynająca się w komórce $(i+1, j+1)$ i mająca rozmiar $m \times m$ - jest zdeterminowana tylko i wyłącznie poprzez koszt $\delta(i,j)$ oraz wektory inicjalne $\delta(i,j+1)...\delta(i,j+m)$ oraz $\delta(i+1,j)...\delta(i+m,j)$. Obliczając wartość w komórce $(i,j)$ odwołujemy się bowiem tylko do trzech sąsiednich komórek $(i-1,j-1), (i-1,j), (i, j-1)$. Możemy więc wcześniej dla każdej możliwej podmacierzy $(i,j,m)$ obliczyć i zapamiętać jej wektory finalne: $\delta(i+m,j+1)...\delta(i+m,j+m)$ oraz $\delta(i+1,j+m)...\delta(i+m,j+m)$.\\

Aby tego dokonać, musimy być w stanie wyenumerować wszystkie możliwe macierze rozmiaru $m$. Zakładamy, że alfabet jest skończony, więc słów długości $m$ jest skończona ilość. Jednak generowanie wszystkich możliwych wektorów inicjalnych może zająć zbyt dużo czasu - wartości komórek mają tendencję do wzrostu wraz z rozszerzaniem się macierzy głównej. Kiedy jednak ograniczymy funkcję kosztu do przypisywania operacjom edycyjnym wyłącznie wielokrotności jakiejś liczby rzeczywistej, okaże się, że mamy tylko skończoną ilość różnic pomiędzy sąsiednimi komórkami w macierzy edycyjnej.\\\\
Będziemy więc operować na wektorach różnic (kroków) pomiędzy komórkami sąsiadującymi ze sobą w poziomie lub pionie. Prowadzi to do następującej modyfikacji twierdzenia 2:\\

\textbf{Twierdzenie 2b}\\\\
$\delta_{i, j} - \delta_{i-1, j} =$\\
$min(R_{A_i,B_j} - (\delta_{i-1,j} - \delta_{i-1,j-1}), D_{A_i}, I_{B_j} + (\delta_{i,j-1} - \delta_{i-1,j-1}) - (\delta_{i-1,j} - \delta_{i-1,j-1}))$\\\\
$\delta_{i, j} - \delta_{i, j-1} =$\\
$min(R_{A_i,B_j} - (\delta_{i,j-1} - \delta_{i-1,j-1}), D_{A_i} + (\delta_{i-1,j} - \delta_{i-1,j-1}) - (\delta_{i,j-1} - \delta_{i-1,j-1}), I_{B_j})$\\\\

Proponowany w pracy \textbf{Algorytm Y} wykonuje pierwszy krok - generuje wszystkie możliwe pary $C, D$ słów długości $m$ oraz wszystkie możliwe wektory inicjalne różnic:  $R$ - pomiędzy sąsiednimi wierszami (wertykalny) i $S$ - pomiędzy sąsiednimi kolumnami (horyzontalny). Następnie oblicza odpowiadające danej podmacierzy wektory finalne różnic - $R'$ oraz $S'$, wyliczając kolejne wektory $T$ (wertykalne) i $U$ (horyzontalne), zgodnie ze wzorem z twierdzenia 2b. Obliczenie i zaindeksowanie każej możliwej podmacierzy zajmuje $O(m^2log m)$ czasu.\\

Następnie \textbf{Algorytm Z} łączy ze sobą podwyniki wygenerowane przez Algorytm Y i oblicza rzeczywistą odległość edycyjną. W tym celu dynamicznie oblicza kolejne finalne wektory dla podmacierzy $(i,j,m)$ dla $i \in \{0,m,...|A|*(m-1)\}$ oraz $j \in \{0,m,...|B|*(m-1)\}$, wykorzystując fakt, że wektor finalny danej podmacierzy jest wektorem inicjalnym dla sąsiadującej (odpowiednio pionowym lub poziomym). Rzeczywista odległość edycyjna może być następnie uzyskana jako suma wartości komórek na dowolnej ścieżce od komórki początkowej do końcowej (dodając kolejne różnice, docierając do ostatniej komórki otrzymamy jej wartość). Czas działania tej części to $O(|A| \cdot |B| \cdot (m+log|A|)/m^2)$. Jeśli wybierzemy $m = \lfloor log_k|A| \rfloor$, całość (Algorytm Y i Z) działa w czasie $O(|A| \cdot |B|/m)$.

\paragraph{Najdłuższy wspólny podciąg}

Powyższego algorytmu można również użyć do obliczenia najdłuższego wspólnego podciągu (LCS). W tym celu jako funkcje kosztu wybieramy:
$D_a = I_a = 1$ oraz $R_{a,b} = 0$ jeśli a=b oraz $R_{a,b} = 2$ w przeciwnym wypadku. Najtańszą ścieżką edycyjną jest w takim przypadku usunięcie z jednego słowa znaków z nie-LCS i dodanie do drugiego słowa znaków z LCS. Do odtworzenia LCS można wykorzystać pomysł analogiczny do odtwarzania ze standardowego algorytmu (Wagnera-Fischera). Zaczynając od ostatniej komórki w macierzy, patrzymy skąd otrzymaliśmy daną wartość i cofamy się. Za każdym razem kiedy operacją, którą wykonywaliśmy jest zamiana, a znaki się zgadzają - mamy do czynienia z literą z LCS. W przypadku algorytmu prezentowanego w pracy nie mamy obliczonej całej macierzy, ale możemy odtworzyć z wektorów inicjalnych podmacierze różnic przecinane przez ścieżkę. Wybór znaków należących do LCS będzie taki sam jak w przypadku rzeczywistej macierzy kosztów.

\section{Najdłuższy wspólny podciąg}

\begin{algorithm}[H]
    \caption{Najdłuższy wspólny podciąg}
    \Input{Zbiór słów $T = \{t_1, t_2, ..., t_k\} \subseteq \A^+$ takich, że $|t_i| \le n$ dla $1 \le i \le k$.}
    \Output{Słowo $t$ będące najdłuższym podciągiem wszystkich słów z $T$.}
\end{algorithm}

Problem najdłuższego wspólnego podciągu jest powiązany z problemem odległości edycyjnej.
Dla $|T| = 2$ niech $d(t_1, t_2)$ oznacza odległość dla dopuszczalnych operacji \emph{insert} i \emph{delete}.
Wówczas zachodzi
\begin{align*}
    |t_1| + |t_2| = 2 |t| + d(t_1, t_2),
\end{align*}
wystarczy bowiem zauważyć, że $t_1$ w $t$ przeprowadzają wszystkie operacje \emph{delete} a $t$ w $t_2$ -- \emph{insert}.

\subsection{Algorytm Needlemana-Wunscha}

Podstawowy algorytm obliczania najdłuższego wspólnego podciągu dla dwóch słów jest prostym rozszerzeniem algorytmu \ref{alg:edit-distance}. Jedyna różnica polega na konieczności trzymania całej tablicy odległości.
Odtwarzanie wyniku następuje na podstawie przejścia po ścieżce od ostatniej wartości tj. $d(t_1, t_2)$ do początku, odpowiadającemu $d(\epsilon, \epsilon)$.
Wybieramy rzecz jasna tylko te przejścia, które spełniają relację $\min$ w równaniu rekurencyjnym.

Oczywiście można odtwarzać kolejne przejścia i uzgodnione litery na podstawie $t_1$, $t_2$ porównując odpowiednie wartości macierzy $d$ -- lub też można zapisywać odpowiednie informacje równolegle w trakcie wypełniania tablicy $d$.

\begin{code}
\captionof{listing}{Obliczanie najdłuższego wspólnego podsłowa w czasie i pamięci $O(n_1 n_2)$}
\inputminted{python}{code/approximate-string-matching/needleman-wunsch.py}
\label{alg:lcs-needleman-wunsch}
\end{code}

\subsection{Algorytm Hirschberga}

Ponieważ dla dostatecznie dużych $n_1$ i $n_2$ w praktyce pamięć jest bardziej dotkliwą barierą niż czas, można zastanowić się nad usprawnieniem algorytmu, aby wykorzystywał tylko $O(\min\{n_1, n_2\})$ pamięci.

Okazuje się, że wystarczy zastosować podejście dziel-i-rządź. Z jednej strony jasne jest, że odpowiadająca najdłuższemu podciągowi optymalna ścieżka od punktu $(0, 0)$ do $(n_1, n_2)$ w macierzy $d$ musi przejść przez pewien punkt $(i, j)$ dla dowolnego $0 \le i \le n_1$. Możemy zatem wybrać $i$ w połowie aktualnie rozpatrywanego przedziału.

Znalezienie $j$ nie jest trudne. Okazuje się, że wystarczy do tego obserwacja z poniższego lematu:
\begin{problem}{}{}
  Punkt $(i, j)$ leży na (pewnej) optymalnej ścieżce w macierzy $d$ wtedy i tylko wtedy, gdy
  \begin{align*}
      j = \arg\max \{0 \le k \le n_2: d(t_1[1..i], t_2[1..k]) + d(\bar{t_1[i..n_2]}, \bar{t_2[k..n_2]})\}.
  \end{align*}
\end{problem}

\begin{code}
\captionof{listing}{Obliczanie najdłuższego wspólnego podsłowa w czasie $O(n_1 n_2)$ i pamięci $O(\min\{n_1, n_2\})$}
\inputminted{python}{code/approximate-string-matching/hirschberg.py}
\label{alg:lcs-hirschberg}
\end{code}

\subsection{Algorytm Myersa}

Algorytm omówiony w pracy służy do poszukiwania najdłuższego wspólnego podciągu. Powstał w oparciu o pracę \textbf{An O(ND) Diffrence Algorithm and Its Variations} autorstwa \textbf{Eugene'a W.Myers}.
\paragraph{Definicje}
Zanim przejdziemy do algorytmu zdefiniujmy graf edycji, definicje wspólnego podciągu oraz pewne pomocnicze określenia.

\paragraph{Graf edycji}
 Mając dane słowa $A = a_{1}a_{2}\dots a_{N}$ oraz $B = b_{1}b_{2}\dots b_{M}$ jako graf edycji G oznaczamy skierowany graf, dla którego zbiór wierzchołków to punkty siatki $(x,y),\:x\in[0,N]\:i\:y\in[0,M]$, natomiast na krawędzi składają się:
\begin{itemize}
    \item $(x-1,y) \xrightarrow{}(x,y)$ dla każdego $x\in[1,N]$ oraz $y\in[0,M]$.
    \item $(x,y-1) \xrightarrow{}(x,y)$ dla każdego $x\in[0,N]$ oraz $y\in[1,M]$.
    \item $(x-1,y-1) \xrightarrow{}(x,y)$ jeśli $a_{x} = b_{y}$, taka krawędź definiuje pewne dopasowanie pomiędzy wzorcami, punkty dla których $a_{x} = b_{y}$ nazwiemy punktami dopasowania.
\end{itemize}

\begin{definition}{}{Podciąg słowa}

Podciąg słowa A to każde słowo powstałe poprzez usunięcie 0 lub więcej symboli z A. Natomiast wspólny podciąg dwóch słów A i B to podciąg każdego z nich. 
\end{definition}

\paragraph{Graf edycji, a wspólny podciąg}

\begin{definition}{}{}
Śladem długości L nazywamy sekwencje L punktów dopasowania $(x_{1},y_{1}),(x_{2},y_{2})\dots(x_{L}, y_{L})$, gdzie dla każdego $1 \leq i < L:\:x_{i}<x_{i+1} \wedge y_{i}<y_{i+1}$. Jest to pewna sekwencji diagonalnych krawędzi. Ślad możemy powiązać z pewną ścieżka pomiędzy $(0,0)$ a $(N, M)$.
\end{definition}

Każdy ślad grafu edycji definiuje pewien wspólny podciąg słów A i B, który powstaje poprzez odpowiednie symbole zdefiniowane przez punkty dopasowania.

\begin{definition}{}{Odległość edycyjna}
Odległość edycyjna D to ilość operacji edycji które przeprowadzamy na tekście A żeby przeprowadzić go w tekst B. Jeśli spojrzymy na ścieżkę która zawiera pewien ślad L to każda krawędź pozioma w tej ścieżce definiuje pewną operację usunięcia symbolu z tekstu A, natomiast krawędzi pionowe definiują operację dodania symboli do tekstu A za odpowiednim indeksem w ciągu A. Możemy zauważyć że wartość odległości edycyjnej jest powiązana z długością śladu L, tj $D = N + M - 2L$.
\end{definition}

Idea algorytmu będzie opierała się na wyszukiwaniu najdłuższego śladu pomiędzy punktami (0,0) oraz (N,M), czyli znalezieniu ścieżki która zawiera największą ilość krawędzi diagonalnych, powyższy ślad wyznaczy nam najdłuższy wspólny podciąg słów A i B.

Pozostały nam jeszcze dwie definicję, które będą wykorzystywane w algorytmie:
\begin{definition}{}{D-ścieżka}
Jako D-ścieżkę definiujemy ścieżkę w grafie edycji rozpoczynającą się w punkcie $(0,0)$, posiadającą dokładnie D krawędzi niediagonalnych.
\end{definition}

\begin{definition}{}{k-diagonala}
W celu wprowadzenia algorytmu zdefiniujemy numerację diagonali. K-diagonala, to taka krawędź diagonalna, która składa się z punktów $(x,y)$, takich że $x - y = k$. Zgodnie z tym krawędzi są numerowane od $-M$ do $N$.
\end{definition}

Na podstawię definicji k-diagonali zauważmy, że krawędź pionowa (pozioma) której punkt startowy leży na k-diagonali, posiada drugi koniec krawędzi na (k-1)-diagonali (odpowiednio (k+1)-diagonali dla poziomej). Zbiór diagonali znajdujących się na końcu krawędzi pionowej/poziomej nazywamy ogonem (w przypadku gdy koniec krawędzi nie jest początkiem żadnej diagonali, wtedy ogon nazywamy pustym).

\paragraph{Algorytm: Poszukiwanie najdłuższej ścieżki}
Algorytm będzie opierał się na wyszukiwaniu najdalszych D-ścieżek w k-diagonali. Przed wprowadzeniem pseudokodu, najpierw omówimy pewne właściwości związane z diagonalami oraz ścieżkami

\begin{lemma}{}{}
Każda D-ścieżka musi kończyć się na k-diagonali, takiej że $k \in \{-D, -D + 2, \dots, D-2, D\}$
\end{lemma}

\begin{proof}
Dowód indukcyjny. Dla D = 0 zauważmy, że 0-ścieżka może kończyć się tylko na diagonali 0, ponieważ oba indeksy $(x,y)$ na tej ścieżce muszą być sobie równe (nie możemy wykorzystać żadnej krawędzi poziomej lub pionowej z definicji 0-ścieżki). Następnie zauważmy, że D-ścieżka składa się z pewnej (D-1)-ścieżki, pewnej poziomej/pionowej krawędzi oraz ogona. Z założenia indukcji mamy że ścieżka D-1 kończy się na pewnej k krawędzi gdzie $k \in \{ -D+1, -D + 3, \dots D -3, D - 1\}$, natomiast nowy ogon będzie składał się z pewnej krawędzi $k \pm 1$(bazując na obserwacji z k-diagonalami), stąd zbiór diagonali na których kończy się D-ścieżka to $k_{D} \in \{ -D + 1 \pm 1, -D + 3\pm 1, \dots D -3\pm 1, D - 1\pm 1\}$, czyli mamy $k_{D} \in \{ -D, -D + 2, \dots, D - 2, D\}$.
\end{proof}

\begin{definition}{}{Najdalej sięgająca D-ścieżka}
D-ścieżkę nazywamy najdalej sięgającą w diagonali k, jeśli jest to ścieżka której końcowy punkt ma największy numer kolumny (odpowiednio numer wiersza z definicji k-diagonali) spośród wszystkich D-ścieżek w k diagonali.
\end{definition}

\begin{lemma}{}{Rozbicie najdalej sięgającej D-ścieżki}
Najdalej sięgająca D-ścieżka w diagonali k może być rozbita na najdalej sięgającą (D-1)-ścieżkę kończącą się na diagonali k - 1, krawędź poziomą oraz najdłuższy ogon z tej krawędzi, lub w najdalej sięgającą (D-1)-ścieżkę w diagonali k + 1, krawędź pionową oraz najdłuższy ogon z tej krawędzi.
\end{lemma}

\begin{proof}
Wybranie odpowiedniej najdalszej (D-1)-ścieżki w diagonali k+1/k-1 związane jest ze wcześniej wspomnianą obserwacją na temat ogonu krawędzi poziomej/pionowej. Natomiast w celu udowodnienie faktu że to najdalej sięgająca (D-1)-ścieżka załóżmy że (D-1)-ścieżka składająca się na kompozycję najdalej sięgającej D-ścieżki nie jest najdalej sięgająca w diagonali k, ale możemy zaobserwować że skoro D-ścieżka musi sięgać dalej niż (D-1)-ścieżka, to oznacza że do ogona tej D-ścieżki będziemy mogli się też dostać z końca ogona najdalszej (D-1)-ścieżki przy pomocy odpowiedniej krawędzi niediagonalnej, więc zawsze możemy najdalszą D-ścieżkę rozłożyć na najdalszą (D-1)-ścieżkę.
\end{proof}

Na bazie obu lematów wprowadzamy algorytm obliczania punktu końca najdalej sięgającej D-ścieżki w diagonali k. Zauważmy że odległość edycyjna $D \in [0, M+N]$, więc będziemy kolejno badali możliwe odległości edycyjne, aż dojdziemy do punktu $(N,M)$.

\begin{verbatim}
\caption{Obliczanie odległości edycyjnej}
\begin{algorithmic}
\STATE $V:[0] + [0] * 2(m+n)$
\STATE $V[1] \leftarrow 0$
\FOR{$D \leftarrow 0$ \TO $M+N$}
    \FOR{$k \leftarrow -D$ \TO $D$ in steps of 2}
        \IF{$k= -D$ or $k \neq D$ and $V[k-1] < V[k+1]$}
            \STATE $x \leftarrow V[k+1]$
        \ELSE
            \STATE $x \leftarrow V[k-1] + 1$
        \ENDIF
        \STATE $y \leftarrow x-k$
        \WHILE{$x<N$ and $y<M$ and $a_{x+1}=b_{y+1}$}
            \STATE $V[k] \leftarrow x$ 
        \ENDWHILE
        \IF{$x \geq N$ and $y \geq M$}
            \RETURN D
        \ENDIF
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{verbatim}

\vspace{5mm}

Powyższy algorytm pozwala nam w czasie O((M + N)D) wyznaczyć odległość edycyjną (a co za tym idzie również długość najdłuższego wspólnego podciągu) przy wykorzystaniu O(D) pamięci.

Niestety problem pojawia się przy wyznaczeniu symboli na które składa się najdłuższy wspólny podciąg. Musielibyśmy po każdej iteracji zapisywać tablicę V, w celu wyznaczenia z których punktów przechodziliśmy w tej ścieżce, co sprawiałoby że musielibyśmy wykorzystać $O(D^2)$ pamięci.

W tym celu w następnej części omówimy algorytm bazujący na przeszukiwaniu najdalszych ścieżek, który będzie działał w O(D) pamięci.

\paragraph{Algorytm w liniowej pamięci}

Finalny algorytm na poszukiwanie najdłuższego wspólnego podciągu będzie opierał się na strategii dziel i rządź. Algorytm będzie korzystał z poszukiwania najdalej sięgającej ścieżki z punktu $(0,0)$ oraz odwrotnej ścieżki z punktu $(N,M)$. W celu obliczenia odwrotnej ścieżki będziemy korzystać z poprzedniego algorytmu, z tym że tym razem będziemy szukali ścieżki z $(N,M)$ w kierunku $(0,0)$ odwracając krawędzie z grafu edycji oraz minimalizując wartość Y w przeciwieństwie do maksymalizowania X z poprzedniego algorytmu (natomiast ścieżka w przód liczona jest analogicznie do poprzedniego algorytmu). Przed zaprezentowaniem algorytmu udowodnimy następujący lemat:

\begin{lemma}{}{}{Podział D-ścieżek}
Pomiędzy punktami $(0,0)$ i $(N, M)$ istnieje D-ścieżka wtedy i tylko wtedy gdy istnieje $\ceil*{D/2}$-ścieżka z $(0,0)$ do punktu $(x,y)$ oraz $\floor*{D/2}$-ścieżka z punktu $(u,v)$ do $(M,N)$, taka że
\begin{itemize}
    \item $u+v \geq \ceil*{D/2}$ oraz $x+y \leq N + M - \floor*{D/2}$
    \item $x-y = u-v$ oraz $x \geq u$.
\end{itemize}
\end{lemma}

Generalnie powyższy lemat opiera się na podziale D-ścieżki na pewne 2 ścieżki, jedna ścieżka z $(0,0)$ a druga ścieżka, którą możemy traktować jako odwrotną ścieżkę z $(N,M)$ w kierunku $(0,0)$. Zauważmy że jak rozłożymy D-ścieżkę na 2 takie przeciwne ścieżki, to ogon znajdujący się na końcu ścieżki do przodu będzie pokrywał się z ogonem znajdującym się na końcu ścieżki przeciwnej, będziemy właśnie z tej właściwości korzystać szukając ścieżkę do przodu i ścieżkę od końca, aż nasze ścieżki pokryją się w pewnej diagonali. Właśnie te diagonale z ogona które się pokrywają wyznaczą nam symbole które będą "środkiem" naszego najdłuższego wspólnego podciągu. 

Następnie wykorzystamy strategię dziel i rządź rozdzielimy sobie słowa na podstawie środkowego ogona i wykonamy się rekurencyjnie na naszych podsłowach, największy wspólny podciąg to rekurencyjne wywołanie lewej części słów, znaków z ogona oraz wywołania na prawej części słów.

Algorytm będzie opierał się na odpowiedniej adaptacji poprzedniego algorytmu do przeszukiwania wprzód D-ścieżek oraz wstecznych D-ścieżek, w przypadku gdy dla jakiejś diagonali k ogony ścieżek się pokryją zwracamy odpowiednią odległość edycyjną oraz ogon który znaleźliśmy. W przypadku gdy dla jakiegoś D, D-ścieżka wprzód pokrywa się z (D-1)ścieżką przeciwną, to zwracamy $2D - 1$, natomiast gdy D-ścieżka przeciwna pokrywa się z D-ścieżką wprzód zwracamy $2D$.

Nasz finalny algorytm na znalezienie największego wspólnego podciągu prezentuję się następująco:

\begin{verbatim}
\caption{LCS(A,B,N,M)}
\begin{algorithmic}
\IF{$N > 0$ and $M > 0$}
    \STATE{ Uruchamiamy naszą procedurę szukania środkowego ogona} 
    \STATE{$(X,Y) \xrightarrow{} (U,V)$ to nasz środkowy ogon, D - odległość edycyjna }
    \IF {$D > 1$}
        \STATE $left = LCS(A[1\dots x],B[1\dots y],x,y)$
        \STATE $middle = A[x+1\dots u]$
        \STATE $right = LCS(A[u+1\dots N],B[v+1\dots M],N-u,M-v)$
        \RETURN $left + middle + right$
    \ELSIF {$M > N$}
        \RETURN $A[1\dots N]$
    \ELSE
        \RETURN $B[1\dots M]$
    \ENDIF
\ENDIF
\end{algorithmic}
\end{verbatim}

\paragraph{Złożoność}

Oznaczmy jako $T(P,D)$ czas działania algorytmu gdzie $P = N + M$. Nasz czas działania opisuje następująco zależność

$$ 
T(P,D) \leq 
     \begin{cases}
       \alpha PD + T(P_{1},\ceil{D/2}) + T(P_{2}, \floor{D/2}) &\quad\text{if } D > 1 \\
       \beta P &\quad\text{if } D \leq 1 \\
     \end{cases}
$$


gdzie $P_{1} + P_{2} \leq P$. Korzystając z zależności $\ceil{D/2} \leq \frac{2D}{3}$ dla $D \geq 2$, możemy udowodnić że $T(P,D) \leq 3\alpha PD + \beta P$, co dowodzi że algorytm wykonuje się w $O((M+N)D)$ czasie.
\vspace{4mm}    

Natomiast nasza złożoność pamięciowa, zauważmy, że procedura poszukiwania środkowego ogonu zajmuje $O(D)$ miejsca, ze względu na wykorzystanie dwóch wektorów V, dodatkowo procedura poszukiwania środkowego ogona działa niezależnie od rekurencji, więc na całą rekurencje potrzebujemy miejsca tylko na 2 wektory, natomiast rekurencja składa się z $O(logD)$ poziomów więc zajmuje $O(logD)$ pamięci, stąd mamy złożoność $O(m+n)$ pamięciową.

\subsection{Algorytm Kumara-Rangana}
Algorytm zaprezentowany w pracy opiera się na zastosowaniu techniki \textit{dziel i zwyciężaj}. Pozwala ona znaleźć szukany najdłuższy wspólny podciąg w złożoności $O(n(m - p))$ oraz przy wykorzystaniu liniowego rozmiaru pamięci. W poniższym opisie $A$ oraz $B$ oznaczają słowa o długości odpowiednio $m$ oraz $n$, które są argumentami algorytmu, natomiast $C$ to szukany najdłuższy wspólny podciąg (LCS) o długości $p$. Zapis $T(i:j)$ oznacza podsłowo słowa T z początkiem o indeksie $i$ oraz końcem o indeksie $j$. Przez $\bar{A}$ oznaczane będzie odwrócone słowo $A$. W opisie pracy zakładam że słowa indeksowane są od cyfry $1$.\\
Pierwszą rzeczą, którą wykonuje algorytm jest obliczenie długości najdłuższego wspólnego podciągu. Znając jego długość możemy obliczyć \textit{perfekcyjne cięcie}, które definiowany jest w następujący sposób:
\begin{definition}{}
Parę $(u, v)$ nazywamy \textit{perfekcyjnym cięciem} dla słów A, B jeżeli:
\begin{enumerate}
    \item $(u, v)$ jest prawidłowym cięciem dla słów A, B; to znaczy jeśli $LCS(A, B)$ możemy przedstawić jako $C = C_1C_2$ wtedy $C_1$ to $LCS(A(1:u), B(1:v))$ oraz $C_2$ to $LCS(A(u+1:m), B(v+1:n))$,
    \item $W(A(1:u), B(1:v)) = (m - p)/2$, gdzie $W(A, B) = |A| - |C|$
\end{enumerate}
\end{definition}
Perfekcyjne cięcie pozwala podzielić zadany problem zgodnie z definicją na dwa podproblemy: \textit{LCS(A(1:u), B(1:v))} oraz \textit{LCS(A(u+1:m), B(v+1:n))}, a z otrzymanych wyników utworzyć rozwiązanie.
Głównym problemem algorytmu jest zatem znalezienie perfekcyjnego cięcia. Do rozwiązania tego problemu wykorzystywane są dwie procedury: \textit{calmid} oraz \textit{fillone}. By lepiej zrozumieć ich ideę wprowadźmy najpierw następujące definicje:
\begin{definition}{}{}
$L_i(k)$ oznacza największe $h$ dla którego słowa \textit{A(i:m), B(h:n)} mają najdłuższy wspólny podciąg długości $k$.
\end{definition}
\begin{definition}{}{}
$L_i^*(k)$ oznacza najmniejsze $h$ dla którego słowa \textit{A(i:m), B(h:n)} mają najdłuższy wspólny podciąg długości $k$.
\end{definition}
\noindent W pracy przedstawiona została zależność pomiędzy \textit{perfekcyjnym cięciem (u, v)} a wartościami $L_i(k)$ oraz $L_i^*(k)$. Pozwala ona znaleźć perfekcyjne cięcie słów $A$, $B$ jeśli zna się wartości $L_i(k)$ oraz $L_i^*(k)$ dla odpowiednich $i$, $k$:
\begin{theorem}{}{perf-cut}
Para (u, v) jest \textit{perfekcyjnym cięciem} wtedy i tylko wtedy gdy $L_{u+1}(m - u - w') \geq v \geq L^*_u(u - w)$, gdzie $w = \lfloor\frac{m-p}{2}\rfloor$ oraz $w' = \lceil\frac{m-p}{2}\rceil$.
\end{theorem}
Warto wspomnieć też o zależności pomiędzy $L_i(j)$ oraz $L_i^*(j)$:
\begin{lemma}{}{}
Załóżmy że znane są wartości $L_i(j)$ dla pary odwróconych słów $(A, B)$. Wartości $L_i^*(j)$ dla nieodwróconych słów $A, B$ są wtedy zadane w następujący sposób:
\[
    L_i^*(j)_{(A, B)} = n + 1 - L_{m-i+1}(j)_{(\bar{A}, \bar{B})}
\]
\end{lemma}
Procedura \textit{calmid(A, B, m, n, x, LL, r)} oblicza wartości $L_{m-x+1-j}(j)$ za pomocą procedury \textit{fillone(A, B, m, n, R1, R2, r, s)} i zapisuje je w tablicy $LL$. Zmienna $r$ po wykonaniu funkcji oznacza natomiast największe j dla którego dana wartość jest poprawnie zdefiniowana. Procedura \textit{fillone(A, B, m, n, R1, R2, r, s)} używa dwóch tablic by obliczyć wartości $L_{s+1}(0), L_s(1), L_{s-1}(2), ...$. Obliczenia te wykonywane są iteracyjne, z wykorzystaniem poprzednich wartości. Poprawność tych funkcji oraz dokładne ich działanie opiszę przy dowodzie ich poprawności.
\subsubsection{Opis dowodu poprawności oraz złożoności algorytmu}
Prezentowany w pracy dowód poprawności algorytmu podzielony został na dowód poprawności dla każdej z procedur zaprezentowanych przez autorów. Na początek przedstawmy dowody poprawności dla procedur \textit{fillone} oraz \textit{calmid} których działanie jest kluczowe dla większości pozostałych funkcji algorytmu. Następnie przyjrzymy się metodzie znajdowania \textit{perfekcyjnego cięcia}. Pozostałe procedury działają w sposób dość jasny do zrozumienia.
\paragraph{Procedura \textit{fillone}}
Tak jak zostało już wspomniane procedura \textit{fillone(A, B, m, n, $R_1, R_2$, r, s)} używa dwóch tablic by obliczyć wartości $L_{s+1}(0), L_s(1), L_{s-1}(2), ...$ . Mając zdefiniowaną pierwszą wartość $L_{s+1}(0) = n + 1$, która wynika wprost z definicji $1.2$, sposób w jaki uzyskiwane są kolejne wartości wynika z następującego lematu:
\begin{lemma}{}{}
Niech $h$ będzie największą liczbą taką że $A[i] = B[h]$ oraz $h < L_{i+1}(j - 1)$, wtedy liczba $L_i(j)$ jest definiowana przez $h$ oraz $L_{i+1}(j)$:
$$
y = \left\{ \begin{array}{ll}
h & \textrm{gdy $ L_{i+1} $ nie jest zdefiniowane}\\
L_{i+1}(j) & \textrm{gdy $ h $ nie istnieje}\\
max(h, L_{i+1}(j)) & \textrm{gdy obie wartości są zdefiniowane}
\end{array} \right.
$$
\end{lemma}
W procedurze wykorzystywana jest zmienna lokalna $lower_b$, która odpowiada za to, by nie przekroczyć wartości dla których $h$ oraz $L_{i+1}(j)$ z powyższego lematu dla danego $i, j$ są niezdefiniowane.\\
Dodatkowo wprowadzona zostaje zmienna $over$, która odpowiada za to by przerwać działanie funkcji gdy wszystkie szukane wartości zostały już obliczone.
Aby uzasadnić liniową złożoność pamięciową funkcji, wystarczy zauważyć że jedyne czego używa procedura \textit{fillone} to liniowej wielkości tablice $R_1$ oraz $R_2$, zatem złożoność pamięciowa to $O(n + m)$.
Złożoność czasowa natomiast wynika z tego że maksymalna liczba porównań którą możemy wykonać w pętli \textit{while} to $n$, ponieważ nigdy nie zwiększamy zmiennej $pos_b$, zatem złożoność to $O(n)$.
\paragraph{Procedura \textit{calmid}}
Procedura składa się z dwóch części. Pierwsza to pętla for, która w $i$-tej iteracji odpowiada za wyliczanie wartości $L_{m-i+2-j}(j)$ i zapisanie jej do $R_1[j]$. Przepisywanie wyników z tablicy $R_2$ do tablicy $R_1$ na koniec każdej iteracji zapewnia, że będą znajdowały się tam poprawne wartości dla wszystkich $0 \leq j \leq r$. Poprawność tych wartości wynika z poprawności działania procedury \textit{fillone} \\
Druga część to przepisanie ostatecznych wartości $L_{m-x-j+1}(j)$ które zapisane są w tablicy $R_1(j)$ do zwracanej tablicy $LL(j)$.\\
Podobnie jak przy procedurze \textit{fillone} złożoność pamięciowa oraz czasowa są dosyć proste do uzasadnienia.
Wykorzystanie pamięci to ponownie jedynie linowego rozmiaru tablice $R_1, r_2, LL$, zatem złożoność pamięciowa to $O(n + m)$.\\
Złożoność czasowa to natomiast $(x + 1)$ wywołanie procedury \textit{fillone} w pętli for. Przypominając że złożoność procedury \textit{fillone} wynosi $O(n)$, zatem złożoność czasowa procedury \textit{calmid} to $O((x+1)n)$.
\paragraph{Znajdowanie perfekcyjnego cięcia}
Procedura znajdowania \textit{perfekcyjnego cięcia} polega na dwukrotnym wywołaniu procedury calmid odpowiednio dla odwróconych słów $A, B$ oraz dla nieodwróconych. Po zakończeniu tych wywołań otrzymujemy w tablicach $LL_1, LL_2$ wyliczone wartości odpowiednio: $n + 1 - L_{m-w+1-k}(k)$ dla $k \leq r_1$ oraz $L_{w+k+1}(m-(w+k)-w')$\ $(= L_{m-w'+1-(p-k)}(p-k))$ dla $p-k \leq r_2$. Korzystając z lematu $1.1$ możemy otrzymać z wartości $n + 1 - L_{m-w+1-k}(k)$ odpowiednie wartości by móc przy pomocy twierdzenia 1.1 stwierdzić, że wystarczy dobrać $u = k + w$ oraz $v = LL_1(k)$ by dostać poprawne perfekcyjne cięcie.\\
Złożoność czasowa i pamięciowa to ta sama, która miała procedura calmid, gdyż dodatkowo wykonywana jest jedynie pętla \textit{for} by dobrać odpowiednie $(u, v)$, która ma złożoność $O(r_1) \leq O(n)$.
\paragraph{Złożoność czasowa i pamięciowa algorytmu}
Patrząc na główną procedurę \textit{Longest common subsequence}, która działa zgodnie z techniką \textit{dziel i zwyciężaj} opisaną na początku dokumentu możemy obliczyć złożoność czasową algorytmu.
Mamy do rozpatrzenia dwa przypadki:
\begin{itemize}
    \item Przypadku bazowego: jego rozwiązanie to wywołanie funkcji \textit{calmid} z parametrem $x = m-p$, gdzie $m - p \leq 2$, zatem jest to złożoność $O(n)$ oraz dodatkowe operacje w pętlach \textit{while}, których maksymalnie zostanie wykonanych $O(m)$. Zatem całkowita złożoność przypadku bazowego to $O(n + m)$.
    \item Znajdowanie \textit{perfekcyjnego cięcia} oraz dwa wywołania rekurencyjne: pierwsze to dwa wywołania procedury \textit{calmid} z parametrem $x = w$ lub $x = w'$ odpowiednio, zatem ich złożoność to $O(n(w + 1))$ i $O(n(w' + 1))$. Złożoność ta to inaczej $O(n(m - p))$, ponieważ tak dobrany był parametry $w$ i $w'$. Dodając dwa wywołania rekurencyjne zgodne z \textit{perfekcyjnym cięciem} $(u, v)$ dostajemy złożoność:
    \[
        T(m, n, p) = O(n(m - p)) + T(u, v, u - w) + T(m - u, n - v, m - u - w')
    \]
    która po przeliczeniach okazuje się wynosić $O(n(m - p))$, ponieważ musi tak być w przypadku gdy $m-p \leq 2$, ponieważ $p \leq n$.
\end{itemize}
Jedyną operacją wykonywaną poza procedurą \textit{Longest common subsequence} jest znalezienie na początku algorytmu długości najdłuższego wspólnego podciągu, która również wykonywana jest w czasie $O(n(m - p))$.
Podsumowując całkowita złożoność czasowa algorytmu to:
\[
O(n(m - p)) + O(n + m) + O(n(m - p)) = O(n(m - p))
\]\\
Złożoność pamięciowa algorytmu jest natomiast liniowa względem rozmiaru słów $A$ i $B$, czyli wynosi $O(n + m)$. By uzyskać taką złożoność przy wywołaniach rekurencyjnych zamiast podsłów przekazujemy jedynie indeksy początku i końca podsłowa. Tablice $LL_1$ oraz $LL_2$ wykorzystywane do obliczania i przechowywanych wartości mają zawsze wielkość liniową. Maksymalna głębokość rekursji jaka może wystąpić w algorytmie to $O(log(m-p))$, zatem całkowita złożoność pamięciowa algorytmu pozostaje liniowa.

\subsection{Inne wyniki}

W ramach wyników negatywnych dla problemu odległości edycyjnej wiadomo, że nie istnieje żaden algorytm dla obliczania odległości edycyjnej o złożoności $\Theta(n^{2 - \varepsilon})$ dla dowolnego $\varepsilon > 0$, o ile Strong Exponential Time Hypothesis jest prawdziwa \citep{backurs2015edit}. Twierdzenie to zachodzi nawet dla przypadku alfabetu binarnego \citep{bringmann2015quadratic}.


