\section{Suffix array, BWT and Wavelet tree}
%Suffix array subsection
\subsection{Suffix Array}
Suffix array introduced in \cite{SuffixArray} is a data structure which describes the suffixes of the text. It stores all the suffixes of text sorted in lexicographic order, but there is one trick, it stores only the beginning index of each suffix.

\begin{definition}\cite[p. 986]{cormenSufixArray}
    Given a text $t = a_1,...,a_n$. The suffix array $SA$ of $t$ is defined such that if $SA[i] = j$ , then $t[j]$ is the $i$-th suffix of $t$ in lexicographic order. That is, the $i$-th suffix of $t$ in lexicographic order is $t[SA[i]:]$.
\end{definition}

We add character $\$$ at the end of text which will work as an empty suffix and that character will also be less than any other character.

\begin{table}[H]
\caption*{Visualization of suffix array}
\begin{center}
\begin{tabular}{|c|c|}
\hline
\rowcolor[HTML]{C0C0C0} 
Index & Suffix   \\ \hline
0     & $ananas\$$ \\ \hline
1     & $nanas\$$  \\ \hline
2     & $anas\$$   \\ \hline
3     & $nas\$$    \\ \hline
4     & $as\$$     \\ \hline
5     & $s\$$      \\ \hline
6     & $\$$       \\ \hline
\end{tabular}
$\implies$
\begin{tabular}{|c|c|c|}
\hline
\rowcolor[HTML]{C0C0C0} 
SA-Index & Original index & Suffix   \\ \hline
0        & 6              & $\$$       \\ \hline
1        & 0              & $ananas\$$ \\ \hline
2        & 2              & $anas\$$   \\ \hline
3        & 4              & $as\$$     \\ \hline
4        & 1              & $nanas\$$  \\ \hline
5        & 3              & $nas\$$    \\ \hline
6        & 5              & $s\$$      \\ \hline
\end{tabular}
\end{center}
\end{table}

If we want to store each suffix as strings, it will require at least $\bigO(n^2)$ time and space, which is not optimal. The suffix array can be computed by many methods, such as:
\begin{table}[H]
\caption{Algorithms for computing suffix array}
\begin{center}
\label{table:suffixArrayAlgos}
\begin{tabular}{|c|c|c|}
\hline
\rowcolor[HTML]{C0C0C0} 
Algorithm                    & Time complexity     & Additional space  \\ \hline
Naive algorithm              & $\bigO(n^2 \log n)$ & $\bigO(n^2)$      \\ \hline
KMR algorithm \cite{KMR}         & $\bigO(n \log^2 n)$  & $\bigO(n \log n)$ \\ \hline
KMR \cite{KMR} + radix sort \cite{radixSort}            & $\bigO(n \log n)$    & $\bigO(n \log n)$ \\ \hline
SA-IS algorithm \cite{SA-IS}             & $\bigO(n)$          & $\bigO(n)$        \\ \hline
Kärkkäinen-Sanders algorithm \cite{KS} & $\bigO(n)$          & $\bigO(n)$        \\ \hline
Larsson-Sadakane algorithm \cite{LS}   & $\bigO(n \log n)$          & $\bigO(n)$        \\ \hline
Linear suffix tree algorithm \cite{suffixTree}           & $\bigO(n)$          & $\bigO(n)$        \\ \hline
\end{tabular}
\end{center}
\end{table}

Suffix array is used in many text algorithms. We will use this structure later for computing FM-index, but if we have suffix array for example we can solve string-matching for many patterns faster than naive algorithms. Note that the suffix array can be computed in $\bigO(n)$ running time using algorithms mentioned in table \ref{table:suffixArrayAlgos}.
\newline
\newline

Let us consider the text $t$ of length $n$, it's suffix array $SA$ and patterns $s_1,s_2...$. To solve string-matching using suffix array we will make simple observation which will allow us to use binary search.

\begin{observation}
\label{observation:suffixArrayObservation}
Pattern $s$ exists in text $t$ if and only if there is a suffix of $t$ that have a prefix equals to $s$.
\end{observation}

Having that observation and a suffix array we can use binary search over the suffix array to find such suffix. Due to the definition of suffix array, all suffixes in it are sorted so that we can use binary search.

\begin{minted}[xleftmargin=20pt, linenos]{python}
# t - text, SA - suffix array, n - length of t, s - pattern
# m - length of s
def pattern_exists(t, SA, n, s, m)
    l = 0
    r = n - 1
    while l > r:
        mid = (l+r)//2
        string_to_compare = ''.join(
            t[i] for i in range(SA[mid], min(n, SA[mid] + m + 1)))
        if string_to_compare < s:
            l = mid + 1
        else:
            r = mid
    if  (n - SA[l] < m) or (t[SA[l] : SA[l] + m] != s):
        return False
    return True
\end{minted}

As we can see, the binary search will execute string comparison of length at most $m$, so the time complexity of this function is $\bigO(\log(n)\cdot m)$, which after the summation for all strings gives us $\bigO(\log(n) \cdot \sum m_i)$, which is not optimal because the input size is $\sum m_i$. In that analysis we are skipping the time needed for calculating the suffix array, which can be done using $\bigO(n)$ time. Comparison of suffix and pattern can be stopped on the first index that are different, so we can save some time comparing characters one by one without materializing the suffix, but the worst complexity will be the same.

\subsection{Burrows-Wheeler transform (BWT)}

The Burrows-Wheeler transform (BWT) is a transformation that permutes the characters of a string in a specific order. The easiest way to show how BWT work is to use an example.
\newline \newline
Let us consider text $t = baabb$. We can mark the end of the text with \$ to make it reversible, so $t = baabb\$$. Now let us construct a matrix of all the circular shifts of the text (circular shift on text is an operation that takes the first character from text and place it at the end of the text).
\bigskip
\newline
$$
\begin{bmatrix}
baabb\$\\
aabb\$b\\
abb\$ba\\
bb\$baa\\
b\$baab\\
\$baabb
\end{bmatrix}
$$
Now we sort the texts in the matrix in lexicographic order. The matrix after sort will look like this:
$$
\begin{bmatrix}


\$baabb\\
aabb\$b\\
abb\$ba\\
baabb\$\\
bb\$baa\\
b\$baab
\end{bmatrix}
$$

The output of BWT is the last column of this matrix, that is, $BWT(t) = bba\$ab$. 
\begin{definition}
    $BWT(t)$ is the last column of the matrix of circular shifts, sorted lexicographically.
\end{definition}
Such definition immediately provide an algorithm for computing $BWT(t)$, which is, get all circular shifts of $t$, sort them and get the last column of matrix. Unfortunately, a direct approach is very slow, because sorting such a matrix in the worst case requires $\bigO(n^2 \log n)$ time, but it can be done much faster. This algorithm works similar to the naive algorithm for computing the suffix array because, the matrix after sort is like a suffix array (but it does not include the indexes), in detail the characters in the first column are the letters on indexes from suffix array. All the texts in this matrix are circular shifts, so the character in the last column is a previous character in text of the character in the first column. This follows to the new observation.

\begin{observation}

$BWT(t)$ on index $i$ is the same as $t[SA[i] - 1]$ or $\$$ if $SA[i] = 0$, and at the first index of $BWT(t)$ is last character of $t$.
\end{observation}

This observation provides a fast algorithm for computing $BWT(t)$, that requires a precomputed suffix array.

\begin{minted}[xleftmargin=20pt, linenos]{python}
def BWT(SA, t, n):
    bwt = [t[n - 1]]
    for i in range(0, n):
        if SA[i] == 0:
            bwt.append('$')
        else:
            bwt.append(t[SA[i] - 1])
    return ''.join(bwt)
\end{minted}

It can be proved that this algorithm requires only $\bigO(n)$ time, which means that it is optimal for computing $BWT(t)$.

\subsection{Wavelet tree}
\label{subsection:WaveletTree}
Wavelet tree is a data structure for performing a certain set of operations on ranges for a given text. In particular, the required operations are:
\begin{itemize}
    \item $\texttt{rank(i, j, x)}$ -- get the number of occurrences of character $x$ in $t[i : j+1]$.
    \item $\texttt{select(i, j, k, x)}$ -- get the $k$-th occurrence of character $x$ in $t[i : j+1]$.
    \item $\texttt{quantile(i, j, k)}$ --  get the $k$-th smallest character in $t[i : j+1]$.
    \item $\texttt{range\_count(i, j, x, y)}$ -- get the number of positions $k \in [i,j]$ such that $x \leq t[k] \leq y$.
\end{itemize}

The wavelet tree can be constructed in $\bigO(n\cdot \log(|\mathcal{A})|)$ time and each of the queries mentioned above can be answered in $\bigO(\log(|\mathcal{A}|))$ time. The only weakness of wavelet tree is its space complexity, equal to $\bigO(n\cdot \log(|\mathcal{A}|))$.

\subsubsection{Idea}
The idea of Wavelet Tree is quite simple. We will represent Wavelet Tree as a binary tree. In each node we store the alphabet of $t$, minimal and maximal character in alphabet, prefix sum array and the information about each character and index to which division does it belong.


For each node we sort an alphabet and divide it in half denoted as $\mathcal{A}_1, \mathcal{A}_2$ and create texts $t_1,t_2$ where $t_1$ (respectively, $t_2$) is a copy of $t$ with all characters for alphabet $\mathcal{A}_2$ (respectively, $\mathcal{A}_1$) removed, but the order of characters exactly preserved as in $t$. Next we recursively create left node with arguments $t_1,\mathcal{A}_1$ and right node with arguments $t_2, \mathcal{A}_2$, until we reach the base case, when the alphabet $\mathcal{A}$ contains only one element. Additionally, in each node we store a prefix sum array where the character contributes to the prefix sum as $0$ where character is in $\mathcal{A}_1$ and as $1$ if character is in $\mathcal{A}_2$.

\begin{tikzpicture}[>=latex]
\matrix[mymat,row 2/.style={nodes=draw}]
at (3,0) 
(mat1)
{
0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
a & b & r & a & k & a & d & a & b & r & a \\
};
\matrix[mymat,row 2/.style={nodes=draw}]
at (0,-3) 
(mat3)
{
0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
a & b & a & a & d & a & b & a \\
};
\matrix[mymat,row 2/.style={nodes=draw}]
at (7, -3)
(mat4)
{
0 & 1 & 2 \\
r & k & r \\
};
\matrix[mymat,row 2/.style={nodes=draw}]
at (-1, -6)
(mat5)
{
0 & 1 & 2 & 3 & 4 & 5 & 6\\
a & b & a & a & a & b & a \\
};
\matrix[mymat,row 2/.style={nodes=draw}]
at (3, -6)
(mat6)
{
0 \\
d \\
};

\matrix[mymat,row 2/.style={nodes=draw}]
at (6, -6)
(mat7)
{
0\\
k\\
};

\matrix[mymat,row 2/.style={nodes=draw}]
at (8, -6)
(mat8)
{
0 & 1 \\
r & r \\
};
\matrix[mymat, row 2/.style={nodes=draw}]
at (-2, -9)
(mat9)
{
0 & 1 & 2 & 3 & 4 \\
a &  a & a & a  & a \\
};
\matrix[mymat, row 2/.style={nodes=draw}]
at (2, -9)
(mat10)
{
0 & 1\\
b & b\\
};

\node[above=0pt of mat1]
  (cella) {Example of Wavelet Tree for $t = abrakadabra$};

\begin{scope}[shorten <= -2pt]

\draw[*->]
  (mat1-2-4.south) -- (mat3-1-4.north);
\draw[*->]
  (mat1-2-8.south) -- (mat4-1-2.north);
\draw[*->]
  (mat3-2-4.south) -- (mat5-1-3.north);
\draw[*->]
  (mat3-2-6.south) -- (mat6-1-1.north);
\draw[*->]
  (mat4-2-1.south) -- (mat7-1-1.north);
\draw[*->]
  (mat4-2-3.south) -- (mat8-1-2.north);
\draw[*->]
  (mat1-2-4.south) -- (mat3-1-4.north);
\draw[*->]
  (mat5-2-3.south) -- (mat9-1-2.north);
\draw[*->]
  (mat5-2-6.south) -- (mat10-1-2.north);

\end{scope}

\end{tikzpicture}

To save time we can sort the alphabet only once in the root node, split it and pass it as an argument for children nodes, which will result in total $\bigO(|\mathcal{A}|\log (|\mathcal{A}|))$ operations for sorting.
\newline
\newline
A construction of Wavelet Tree can be implemented in the following way:

\begin{minted}[xleftmargin=20pt, linenos]{python}
class WaveletTree:
  def __init__(self, t, n, sorted_alphabet = None):
    t = t[1:]
    if sorted_alphabet is not None:
      self.alphabet = set(sorted_alphabet)
    else:
      self.alphabet = set(t)
      sorted_alphabet = sorted(list(self.alphabet))
    self.n = n
    self.smallest = sorted_alphabet[0]
    self.largest = sorted_alphabet[-1]
    if len(sorted_alphabet) == 1:
      self.leaf = True
      return
    self.leaf = False
    left_alphabet = sorted_alphabet[:(len(sorted_alphabet) + 1)//2]
    right_alphabet = sorted_alphabet[(len(sorted_alphabet) + 1)//2:]
    self.zero_indexed = set(left_alphabet)
    self.one_indexed = set(right_alphabet)
    value_array = [1 if c in self.one_indexed else 0 for c in t ]
    self.prefix_sum = [0]
    for i in range(n):
      self.prefix_sum.append(self.prefix_sum[i] + value_array[i])
    self.left_indices = [0]
    self.right_indices = [0]
    for i in range(n):
      if t[i] in self.zero_indexed:
        self.left_indices.append(i+1)
      else:
        self.right_indices.append(i+1)
    left_t = ['#'] + [c for c in t if c in self.zero_indexed]
    right_t = ['#'] + [c for c in t if c in self.one_indexed]
    self.left = WaveletTree(left_t, len(left_t) - 1, left_alphabet)
    self.right = WaveletTree(right_t, len(right_t) - 1, right_alphabet)
\end{minted}

It can be seen that the creation of Wavelet Tree requires $\bigO(n \log(|\mathcal{A}|))$ time and space. We can observe that for each level of nodes the total time and space is the same, and it is equal to $\bigO(n)$ also the height of the tree is $\bigO(\log(|\mathcal{A}|))$. The creation complexity can be compared to the complexity of Merge Sort \cite[p. 34-44]{cormenSufixArray} where we use similar approach. Also note that $|\mathcal{A}|$ is less or equal to $n$, so the time used for the sorting at the root node will not be greater than $\bigO(n \log(|\mathcal{A}|))$.
\newline

Using all the information stored in Wavelet Tree we can now proceed to answer the queries.
\newline

\subsubsection{Rank query}
$\texttt{rank(i, j, x)}$ query can be answered using recursion and modifying the range as we go deeper into the tree to the leaf, it can be described in the following way:

\begin{enumerate}
    \item If $x$ is not in the alphabet or range $[i,j]$ is empty return $0$.
    \item If the node is a leaf, then return $r-l+1$.
    \item Modify $i$ and $j$, so they correspond to the correct positions in a recursive call.
    \item Recursive call on proper child and range.
\end{enumerate}

The range modification may be implemented in the following way:
\begin{minted}[xleftmargin=20pt, linenos]{python}
  def _left_tree_range(self, l, r):
    return l - self.prefix_sum[l-1], r - self.prefix_sum[r]

  def _right_tree_range(self, l, r):
    return (self.prefix_sum[l-1] + 1, self.prefix_sum[r])
\end{minted}

The correctness of each query follows explicitly from the correctness of range mapping and the trivially true base case.

For the right node we observe that the prefix sum correspond to the number of elements in right tree at given index, so new indexes in right tree are just values of prefix sum. In the other case the indices in left node are the complement for right node, so it will be original index minus the prefix sum value. Using such functions, the $\texttt{rank(i, j, x)}$ query can be implemented in the following way:

\begin{minted}[xleftmargin=20pt, linenos]{python}
  def rank(self, c, l, r):
    if c not in self.alphabet or l > r or l > self.n or r < 1:
      return 0
    if self.leaf:
      return r-l+1
    if c in self.zero_indexed:
      new_l, new_r =  self._left_tree_range(l, r)
      return self.left.rank(c, new_l, new_r)
    new_l, new_r =  self._right_tree_range(l, r)
    return self.right.rank(c, new_l, new_r)
\end{minted}

Note that it is easy to modify $\texttt{rank(c, l, r)}$ function to return all indices. The only difference is that we return range $[l, r]$ in the base case and map all indices from the recursion call to current node indices.

\subsubsection{Select query}
Answering the $\texttt{select(i, j, k, x)}$ query uses the same approach as the $\texttt{rank(i, j, x)}$ query but returns another information in the base case where we just return an index and as we leave the recursion call we map it to an index in currently considered text inside a node. It may be implemented as follows:

\begin{minted}[xleftmargin=20pt, linenos]{python}
  def select(self, c, k, l, r):
    if c not in self.alphabet or l > r or l > self.n or r < 1 :
      return None
    if self.leaf:
      return k+l-1 if k <= r-l+1 else None
    if c in self.zero_indexed:
      new_l, new_r =  self._left_tree_range(l, r)
      rec_result = self.left.select(c, k, new_l, new_r)
      return (self.left_indices[rec_result] if rec_result is not None 
        else None)
    new_l, new_r =  self._right_tree_range(l, r)
    rec_result = self.right.select(c, k, new_l, new_r)
    return (self.right_indices[rec_result] if rec_result is not None
        else None)
\end{minted}

\subsubsection{Quantile query}
The $\texttt{quantile(i, j ,k)}$ will also use similar approach but in each step we will count the number of characters in the left node in the corresponding range and recursively call the function according to the $k$ and number of characters. As can be seen if number of characters in the left node in range $[i, j]$ is less than $k$ we will call recursively with arguments $mapped(i)$, $mapped(j)$ and $k - num$, due to that each character in left node are smaller than in the right node, and now we are searching for $(k-num)$-th element, in other case we call recursively $mapped(i)$, $mapped(j)$, $k$ on the left node. As for the base case we will just return the character which is in the leaf.

\begin{minted}[xleftmargin=20pt, linenos]{python}
  def quantile(self, k, l, r):
    if k < 1 or k > r-l+1:
      return None
    if self.leaf:
      return self.smallest if k <= self.n else None
    left_num = self.prefix_sum[r] - self.prefix_sum[l-1]
    if r-l+1-left_num >= k:
      new_l, new_r =  self._left_tree_range(l, r)
      return self.left.quantile(k, new_l, new_r)
    new_l, new_r =  self._right_tree_range(l, r)
    return self.right.quantile(k-r+l-1+left_num, new_l, new_r)
\end{minted}

All the queries requires constant time in a single node and use one recursion call in one of the child, so the time complexity is proportional to the depth of the tree, which is $\bigO(\log(|\mathcal{A}|))$. 

\subsubsection{RangeCount query}
As for $\texttt{range\_count(i, j, x, y)}$ query here we use another approach. For each range that fulfills condition $x \leq node.smallest$ and $node.biggest \leq y$ the answer is simple, it is $j-i+1$. So we want to find the closest nodes that fulfill that condition and sum the answers. To find such nodes, we can use recursion in the following way (the order of conditions is important):

\begin{enumerate}
    \item Base case: if $x \leq node.smallest$ and $node.biggest \leq y$, then return (j - i + 1)
    \item If the node is a leaf, then return 0
    \item If $[x, y]$ intersect with range of elements of left child and right child, then use recursive call on both children and return the sum of results.
    \item If $[x, y]$ intersect only in the right child range of elements, then return the result of recursion call for right child.
    \item Otherwise, return the result of recursion call for left child.
\end{enumerate}



The intersection of ranges can be implemented naively, just considering all cases:

\begin{minted}[xleftmargin=20pt, linenos]{python}
  def _does_one_range_end_in_another(self, l, r, i, j):
    return (i <= l <= j) or (i <= r <= j)
    
  def _ranges_intersect(self, l, r, i, j):
    return (self._does_one_range_end_in_another(l, r, i ,j) or
      self._does_one_range_end_in_another(i, j, l, r))
\end{minted}

The first case is just a base case, so we return the length of the range. In the second case if the current node is leaf, then it means that no element fulfills the condition, so we terminate the recursion here. As for other case we just check if there are some elements in each child and invoke a proper recursion call. To prove the time complexity we need the following fact:

\begin{observation}
    After the first call of case $3$ for the next calls of case $3$ one of its child will fulfill base case condition.
\end{observation}

\begin{proof}
    We inductively assume that this property holds for the children of the node. Each element of left child is smaller than any element of the right child. That implies that for each case $3$ which occurs in the left child, the recursion call on the right child will fulfill the base case due to the fact that earlier in one of the ancestors of that node we used recursion call for right node, but all elements there were greater than in current child. Hence, that $node.right.biggest \leq y$ and also we used recursion call for the left side, but it can only happen if $x \leq node.left.biggest$, which implies that $x \leq node.right.smallest$, and it is exactly the base condition. The analysis for case $3$ calls on the nodes on the right after the first occurrence of case $3$ is similar.
\end{proof}

Using that observation we can now calculate properly the time complexity of \\ $\texttt{range\_count(i, j, x, y)}$. If case $3$ did not occur at all during the query the answer is simple, it is just the height of tree which is $\bigO(\log(|\mathcal{A}|))$. In the other case the first case $3$ occurs after at most $\bigO(\log(|\mathcal{A}|))$ operations, and now we can calculate the time needed for left and right child. On each of the next occurrences of case $3$ we can observe that it will use $\bigO(1)$ operations for at least one of the child and in the other child in the worst case the recursion can go deeper which also can be represented as height of the tree which is $\bigO(\log(|\mathcal{A}|))$. In the worst case we need $\bigO(\log(|\mathcal{A}|))$ operations for the first occurrence and $\bigO(\log(|\mathcal{A}|))$ operations for both children which in total results in $\bigO(\log(|\mathcal{A}|))$ running time. The implementation of that function is shown below:

\begin{minted}[xleftmargin=20pt, linenos]{python}
def range_count(self, l, r, x, y):
  if l > r or l > self.n or l < 1 or x > y:
    return 0
  if x <= self.smallest  and self.largest <= y:
    return r-l+1
  if self.leaf or y < self.smallest or x > self.largest:
    return 0
  l_node, r_node = self.left, self.right
  if (self._ranges_intersect(l_node.smallest, l_node.largest, x, y) and
      self._ranges_intersect(r_node.smallest, r_node.largest, x, y)):
    new_left_l, new_left_r = self._left_tree_range(l, r)
    new_right_l, new_right_r = self._right_tree_range(l, r)
    return (self.left.range_count(new_left_l, new_left_r, x, y) +
      self.right.range_count(new_right_l, new_right_r, x, y))
  if (self._ranges_intersect(self.right.smallest,
      self.right.largest, x, y)):
    new_l, new_r = self._right_tree_range(l, r)
    return self.right.range_count(new_l, new_r, x, y)
  new_l, new_r = self._left_tree_range(l, r)
  return self.left.range_count(new_l, new_r, x, y)
\end{minted}

Note that $\texttt{rank(c, l, r)}$ and $\texttt{range\_count(l, r, x, y)}$ functions are easy to modify to return all indices. The only difference is to return range $[l, r]$ in the base case and map all indices from the recursion call to current node indices. Modification of $\texttt{range\_count(l, r, x, y)}$ returning all indices will be called $\texttt{range\_search(l, r, x, y)}$ and will be useful in construction of LZ-Index.