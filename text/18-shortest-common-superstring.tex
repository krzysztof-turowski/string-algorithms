\section{Najkrótsze wspólne nadsłowo}

\begin{algorithm}[H]
    \caption{Najkrótsze wspólne nadsłowo}
    \Input{Zbiór słów $T = \{t_1, t_2, \ldots, t_k\} \subseteq \A^+$ takich, że $|t_i| \le n$ dla $1 \le i \le k$.}
    \Output{Słowo $t$ będące najkrótszym nadsłowem wszystkich słów z $T$.}
\end{algorithm}

Bez straty ogólności zakładamy, że dla żadnej pary słów $t_i$ i $t_j$ nie zachodzi sytuacja, że $t_i$ jest podsłowem $t_j$. Gdyby taka sytuacja wystąpiła, to moglibyśmy usunąć $t_i$ z $T$ bez zmiany optymalnego rozwiązania.

W ogólności problem jest trudny, zarówno dla krótkich słów, jak i dla małych alfabetów.
\begin{theorem}{gallant1980finding}{}
  Niech $T = \{t_1, t_2, ..., t_k\}$ i $|t_i| = 3$ dla $1 \le i \le k$.
  Problem znalezienia najkrótszego nadsłowa dla $T$ jest NP-trudny.
\end{theorem}

\begin{theorem}{gallant1980finding}{}
  Niech $T = \{t_1, t_2, ..., t_k\}$ oraz $|\A| = 2$.
  Problem znalezienia najkrótszego nadsłowa dla $T$ jest NP-trudny.
\end{theorem}

Pokazano, że problem znalezienia najkrótszego nadsłowa należy do klasy złożoności MAX-SNP, a zatem istnieje stała $c > 1$, dla której nie istnieje algorytm $c$-przybliżony dla tego problemu. \citet{vassilevska2005explicit} dowiodła, że $c \ge \frac{1217}{1216}$, \citet{karpinski2013improved} poprawili to do $c \ge \frac{333}{332}$.

\begin{problem}{apostolico-galil}{Theorem 8.2, s. 239-240}
  Niech $T = \{t_1, t_2, ..., t_k\}$ przy $|t_i| \le 2$ dla $1 \le i \le k$.
  Pokaż, jak możliwe jest wyznaczenie najkrótszego nadsłowa $t$ dla $T$ w czasie wielomianowym.
\end{problem}

\subsection{Algorytm zachłanny}

\todo[inline]{Kod i dowód złożoności}

Niech $T = \{c(ab)^k, (ba)^k, (ab)^kc\}$, wówczas algorytm zachłanny najpierw złoży ze sobą dwa skrajne ciągi, a zatem $t_{APX} = (ba)^kc(ab)^kc$, $|t_{APX}| = 4 k + 2$, podczas gdy $t_{OPT} = c(ab)^{k + 1}c$, $|t_{OPT}| = 2 k + 4$.

Została postawiona hipoteza, że algorytm zachłanny jest $2$-przybliżony, ale dotychczas udało się tylko pokazać współczynnik aproksymacji równy $4$ \citep{blum1994linear}, a następnie poprawić współczynnik go do $3.5$ \citep{kaplan2005approximation}.

Zamiast porównywać długości słowa optymalnego i zwróconego przez algorytm możemy też zmienić kryterium optymalizacji.
Naturalnym alternatywnym kryterium wydaje się wielkość kompresji:

\begin{algorithm}[H]
    \caption{Wspólne nadsłowo o największej kompresji}
    \Input{Zbiór słów $T = \{t_1, t_2, \ldots, t_k\} \subseteq \A^+$ takich, że $|t_i| \le n$ dla $1 \le i \le k$.}
    \Output{Słowo $t$ będące nadsłowem wszystkich słów z $T$ i maksymalizujące $d = \sum_{i = 1}^k |t_i| - |t|$.}
\end{algorithm}
Oczywiście zbiór rozwiązań optymalnych dla obu problemów jest identyczny.

Okazuje się, że przy tych założeniach można pokazać, że algorytm zachłanny jest $2$-przybliżony tj. osiąga kompresję taką, że $2 d_{APX} \ge d_{OPT}$ \citep{tarhio1988greedy}.

\subsection{Algorytm $O(\log{n})$-przybliżony}
Problem najkrótszego nadsłowa w wersji decyzyjnej jest problemem NP zupełnym. W związku z tym pojawiało się wiele podejść do aproksymacji optymalizacyjnej wersji problemu. Tarhio i Ukkonen oraz Turner napisali dwie niezależne prace naukowe, w których opisano algorytmy aproksymacyjne dla rozważanego problemu. Mimo, że autorzy twierdzili, że skonstruowane przez nich algorytmy powinny być $2$-aproksymacyjne, to nie udało im się udowodnić żadnego ograniczenia. 

Algorytm opisany przez Minga Li jest pierwszą aproksymacyją, dla której udało się udowodnić współczynnik - $log(n)$. Autor pracy twierdzi, że współczynnik w rzeczywistości wynosi co najwyżej $2$.
  
\paragraph{Algorytm}
Zakładamy, że podane na wejściu słowa spełniają warunek, iż żadne nie jest podsłowem innego. Możemy tak zrobić, ponieważ wyeliminowanie tego typu sytuacji jest niezmiennicze ze względu na wynik i zajmuje wielomianowy czas.
\newline 

\begin{definition}{}{}
Dla danych słów $s_1, s_2$ słowo $m(s_1, s_2)$, długości nie większej niż $|s_1| + |s_2|$, nazywamy scaleniem, jeżeli $s_1$ jest jego prefiksem, a $s_2$ sufiksem (lub $s_2$ prefiksem, a $s_1$ sufiksem).
\end{definition}

\begin{lemma}{}{}
Dla danych słów $s_1, s_2$ istnieje co najwyżej $2 min(|s_1|, |s_2|)$ scaleń.
\end{lemma}

Dowód powyższego lematu jest trywialny.

\textbf{Algorytm}
\begin{enumerate}
    \item Input to $S = \{s_1, \dots, s_n\}$. Jeżeli $|S| = 1$, to zwróć $s_1$.
    \item Zdefiniujmy $T = \emptyset$.
    \item Niech $s, s'$ będą słowami minimalizującymi 
    $$\min_{m - \text{scalenie}} \frac{|m(s, s')|}{v(m(s, s'))},$$
    gdzie 
    $$v(m(s, s')) = \sum_{a\in A(m(s,s'))} |a|$$ 
    dla $A(m(s,s'))$ będącego zbiorem podsłów $m(s,s')$ z $S$. Wrzucamy $m(s, s')$ do $T$ dla $m$ realizującego powyższe minimum, natomiast wszystkie słowa z $A(m(s,s'))$ wyrzucamy z $S$.
    \item Jeżeli $|S| \leq 1$, to $S = T \cup S$ i przechodzimy do $1$.
    \item W przeciwnym przypadku przechodzimy do $3$.
\end{enumerate}

Złożoność jest oczywiście wielomianowa - przy każdym powrocie do punktu~$1$, moc zbioru $S$ zmniejszyła się przynajmniej dwukrotnie, natomiast sumaryczna długość słów znajdujących się w nim może się tylko zmniejszyć.

Wyszukiwanie minimum i modyfikacja zbiorów w punktach $3$ i $4$ jest trywialnie realizowane w czasie wielomianowym.

Pozostaje ograniczyć współczynnik aproksymacji algorytmu.

\paragraph{Współczynnik aproksymacji}

\begin{theorem}{}{}
Algorytm przedstawiony powyżej jest $log(n)$ aproksymacyjny.
\end{theorem}
\begin{proof}
Niech $s_1, \dots, s_m$ będzie ułożeniem słów wejściowych w kolejności ich występowania w optymalnym rozwiązaniu - jeżeli słowo występuje kilka razy, to rozważamy najbardziej lewe wystąpienie. Dzielimy słowa na następujące grupy:
\newline
Grupa $G_1$ zawiera $s_1, \dots, s_i$, gdzie $s_i$ to ostatnie słowo takie, że pierwsze wystąpienie $s_1$ nachodzi na pierwsze wystąpienie $s_i$ w optymalnym rozwiązaniu.
\newline
Grupa $G_2$ budowana jest w analogiczny sposób, z tym że zaczynamy konstrukcje od $s_{i+1}$. W ten sposób dzielimy całe wejście $s_1, \dots, s_m$ na rozłączne grupy $G_1, \dots, G_k$.

Dla każdej grupy $G_i$ wyróżniamy elementy $b_i, t_i$ - pierwsze i ostatnie słowa (we wcześniej wspomnianym porządku) budujące grupę. Jako że $b_i$ nachodzi na $t_i$, to istnieje scalenie $m_i$ takie, że wszystkie słowa z grupy $G_i$ są podsłowami $m_i(b_i, t_i)$.
Zauważmy, że 
$$\sum_{i=1}^{k} |m_i(b_i, t_i)| \leq 2n,$$
gdzie $n$ to długość optymalnego rozwiązania, ponieważ każda litera słowa optymalnego jest pokrywana przez maksymalnie dwie grupy.

Wprowadźmy teraz pojęcie kosztu słowa w kontekście analizowanego przez nas algorytmu. Załóżmy, że słowo $s$ odpadało ze zbioru $S$ na rzecz scalenia $m(s', s'')$. 
$$cost(s) = \frac{|m(s', s'')|}{v(m(s', s''))} \cdot |s|.$$

Zauważmy, że koszt całego algorytmu (długość słowa wynikowego) jest równa nie więcej niż długość wszystkich słów, które powstały w trakcie pierwszej fazy opróżniania zbioru $S$ - elementów $s_1, \dots, s_m$. Niech $S_h = A(M_h(B_h, T_h))$ to zbiór elementów wyrzuconych $h$-tym kroku wspomnianej fazy. 
$$COST \leq \sum_{h=1}^{r} |M_h(B_h, T_h)| = \sum_{i=h}^{r} \frac{|M_h(B_h, T_h)|}{v(M_h(B_h, T_h))} \cdot v(M_h(B_h, T_h))= $$
$$\sum_{h=1}^{r} \sum_{s \in S_h} \frac{|M_h(B_h, T_h)|}{v(M_h(B_h, T_h))} \cdot |s| = \sum_{h=1}^{r} \sum_{s \in S_h} cost(s) = \sum_{s \in S} cost(s).$$
\end{proof}

Rozważmy teraz grupę $G_j$ i koszt, jaki płacimy za wyrzucenie jej elementów. Grupę $G_j$ dokładnie przed fazą $h$ będziemy oznaczać $G_j^{h}$.
Sumę długości słów w zbiorze $A$ oznaczamy $||A||$.
 

$$Cost(G_j) = \sum_{h=1}^{k} \sum_{s \in (G_j^{h+1} \backslash G_j^{h})} cost(s) = \sum_{h=1}^{k} \frac{|M_h(B_h, T_h))|}{v(M_h(B_h, T_h))} \cdot (||G_j^{h+1}|| - ||G_j^{h}||).$$
Jako że nasz algorytm wybierał scalenia $M_h(B_h, T_h)$ realizujące minimalne wartości, to
$$\sum_{h=1}^{k} \frac{|M_h(B_h, T_h))|}{v(M_h(B_h, T_h))} \cdot (||G_j^{h+1}|| - ||G_j^{h}||) \leq \sum_{h=1}^{k} \frac{|m_j(b_j, t_j))|}{v(m_j(b_j, t_j))} \cdot (||G_j^{h+1}|| - ||G_j^{h}||) = $$
$$\sum_{h=1}^{k} \frac{|m_j(b_j, t_j))|}{||G_j^{h}||} \cdot (||G_j^{h+1}|| - ||G_j^{h}||) \leq |m_j(b_j, t_j))| \cdot Harm(||G_j||).$$
Jako że $n$ jest co najwyżej wielomianowo większe niż każde $||G_j||$, to dostajemy oszacowanie
$$Cost(G_j) \leq |m_j(b_j, t_j))| \cdot log(n),$$
co z poprzednim lematem skutkuje w
$$COST \leq 2nlog(n).$$



\subsection{Algorytm 3-przybliżony}

Mając na wejściu słowa $S = \{s_1,...,s_m\}$, takie, że żadne z nich nie zawiera żadnego innego w całości, możemy poszukać wspólnego nadsłowo $s$ dla danego zbioru $S$.

Najprostszym przykładem może być słowo $s=s_1s_2...s_m$. Jednakże, tak uzyskane nadsłowo będzie niepotrzebnie długie.

Jako, że problem ten ma duże znaczenie praktyczne (w sekwencjonowaniu DNA) naturalnym jest próba znalezienia najkrótszego wspólnego nadsłowa.

Oczywistym podejściem jest następujący algorytm zachłanny. Po kolei łącz słowa mające największą część wspólną nachodzącą na siebie określaną jako \textit{overlap}. Rób tak, aż do uzyskania 
jednego słowa. Postawiona jest hipoteza, że takie podejście ma współczynnik aproksymacji równy 2, ale przed opublikowaniem pracy przez Blum et al. nie było wiadome, czy taki algorytm jest w ogóle liniową aproksymacją. Okazuje się, że ten algorytm (określać będziemy go poprzez \textbf{GREEDY}) ma współczynnik aproksymacji co najwyżej 4.

Ponadto Blum et al. zaprezentowali alternatywny algorytm, nazywany \textbf{TGREEDY}, dla którego udowodnili, że jest on 3-aproksymacyjny. Sam problem znalezienia najkrótszego wspólnego nadsłowa jest NP trudny.

W pierwszej sekcji przedstawimy podstawowe definicje i notacje, a w kolejnych algorytm \textbf{MGREEDY}, a w następynej sekcji algorytm 
\textbf{TGREEDY} wykorzystujący algorytm \textbf{MGREEDY} by uzyskać pożądany współczynnik aproksymacji. W ostatniej sekcji postaramy się udzielić odpowiedzi na pytanie, który z tych dwóch algorytmów jest lepszy? \pagebreak
\paragraph{Podstawowe definicje}

Mając dwa niekoniecznie różne słowa $s = uv, t = vw$, takie, że $u,w$ są niepuste, a $v$ maksymalnej długości, mianem \textit{overlap}'u słów $s,t$ określamy $v$.
Długośc overlap'u słów $s,t$ oznaczamy jako $ov(s,t) = |v|$. Zauważmy, że dla $s=t$ overlap jest maksymalnym właściwym prefikso-sufiksem. 

Ponadto $u$ nazwiemy \textit{prefiksem słowa $s$ względem słowa $t$} i oznaczymy poprzez \linebreak $pref(s,t) = u$. \textit{Odległością między $s$, a $t$} nazwiemy $d(s,t)=|pref(s,t)|$.

Wtedy słowo postaci $uvw = pref(s,t)t$ jest najkrótszym nadsłowem $s$ i $t$ o długości $d(s,t) + |t| = |s| + |t| - ov(s,t)$. Takie słowo będziemy też określali jako
\textit{merge słów $s$ i $t$}. 
Dla słów $s_i, s_j \in S$ będziemy używali 
$pref(i,j), d(i, j), ov(i, j)$ zamiast \linebreak
$pref(s_i,s_j), d(s_i,s_j), ov(s_i, s_j)$.

Mając słowa $s_{i_1}, s_{i_2},...,s_{i_r}$ zdefiniujemy $s = \langle s_{i_1},...,s_{i_r}\rangle$ jako \\ $s = pref(i_1,i_2)...pref(i_{r-1}, i_r)s_{i_r}$, czyli najkrótsze słowo, w którym słowa
$s_{i_1},...,s_{i_r}$ pojawiają się dokładnie w tej kolejności.

Określmy $s_{i_1}$ poprzez $first(s)$ oraz $s_{i_r}$ poprzez $last(s)$.

W trakcie algorytmu \textbf{GREEDY}, a także \textbf{MGREEDY} następujący niezmiennik jest zachowany.

\begin{lemma}{}{}
  
W dowolnym momencie algorytmu \textbf{GREEDY} w aktualnie przetwarzanym zbiorze słów dla każdych dwóch różnych słów $s,t$ z tego zbioru jest, że 
$first(s)$ ani $last(s)$ nie jest podsłowom słowa $t$.

\end{lemma}

\begin{proof}
Udowodnimy lemat dla $first(s)$, dowód dla $last(s)$ jest zupełnie analogiczny.

Początkowo $first(s)=last(s)=s$ dla każdego słowa $s$ w aktualnym zbiorze. Załóżmy, że niezmiennik jest w pewnym momencie naruszony poprzez złączenie dwóch słów 
$t_1,t_2$ w słowo $t = \langle t_1,t_2\rangle$, czyli, że $t$ ma $first(s)$ jako podsłowo. Wtedy wiemy, że 
$t = ufirst(s)v$ dla jakichś $u,v$. Skoro $first(s)$ nie może być podsłowem 
$t_1$ ani $t_2$ (założenie o zbiorze $S$), to $|first(s)| > ov(t_1,t_2)$ oraz 
$|u| < d(t_1,t_2)$. Czyli, $ov(t_1, s) > ov(t_1, t_2)$, co daje nam sprzeczność z faktem, że $ov(t_1,t_2)$ jest największym.
\end{proof}

Lemat ten mówi nam, że algorytm wybierająć dwa słowa $s,t$ o największym $ov(s,t)$ wybiera de facto słowa o największym $ov(first(s), last(t))$.
W wyniku takiego łączenia powstaje słowo $\langle first(s),...,last(s),first(t),...,last(t)\rangle$.
Możemy, więc powiedzieć, że \textbf{GREEDY} porządkuje słowa w ten sposób (dla każdej pary słów porównując ich overlap'y), a następnie 
znajduje najkrótsze wspólne nadsłowo zawierające te słowa w tej kolejności.

Wprowadzimy teraz pojęcie grafu odległości, który opisuje nam naszą instancję problemu. Mianowicie,
$G_S = (V,E)$, gdzie wierzchołkom przypisane są poszczególne słowa z $S$, a krawędziom będą przypisane 
słowa $pref(i,j)$ o wadze $|pref(i,j)|=d(i,j)$.

Możemy na takim grafie łatwo stwierdzić, że jest zachowana nierówność: \linebreak
$CYC(G_S) \leq TSP(G_S) \leq OPT(S) - ov(last(s), first(s)) \leq OPT(S)$.

Dzięki temu można pokazać, że znajdując minmialne pokrycie cyklowe ($CYC(G_S)$) możemy skonstruować wspólne nadsłowo o długości co najwyżej $4 \cdot OPT(S)$ poprzez konkatenacje 
słów wynikających z poszczególnych cyklów w pokryciu cyklowym. Tak opisany algorytm nazywamy \textbf{CONCATCYCLES}.

\paragraph{Algorytm \textit{\textbf{MGREEDY}}}
Możemy teraz opisać algorytm \textbf{MGREEDY}.
\begin{enumerate}
  \item Niech $S$ będzie zbiorem słów początkowych, a $T$ zbiorem pustym.
  \item Dopóki $S$ jest niepusty, wybierz $s,t$ z $S$ maksymalizujące $ov(s,t)$.\\
  Dla $s\neq t$, niech $S := (S \setminus \{s, t\}) \cup \{\langle s, t \rangle\}, T := T$.\\
  Dla $s = t$, niech $S := S \setminus \{s\}$, a $T := T \cup \{s\}$. 
  \item Jako wspólne nadsłowo zwróc konkatenację słów ze zbioru $T$.
\end{enumerate}

Rozważmy teraz graf $G'_S$, analogiczny jak $G_S$, ale dla każdej krawędzi mamy jako wagę $ov(i,j)$ zamiast $d(i,j)$.
Zauważmy, że dla dowonlego pokrycia cyklowego grafu $G_S$ oraz tego samego pokrycia cyklowego grafu $G'_S$ mamy, że 
suma wag z pokrycia cyklowego z tych dwóch grafów równa się sumie długości słów w $S$. Jest tak ponieważ $\forall s,t: ov(s,t) + d(s,t) =  |s|$. Czyli maksymalne pokrycie cyklowe grafu $G'_S$, będzie tożsame z
minimalnym pokryciem cyklowym grafu $G_S$. Okazuje się, że \textbf{MGREEDY} łącząc dwa słowa $s\neq t \in S$ w jedno, de facto 
wybiera krawędź w grafie $G'_S$ między $s_\alpha := last(s)$, a $s_\beta := first(t)$ takie, że $ov(\alpha, \beta)$ jest maksymalne wśród wszystkich wolnych $ov(i,j)$.

Wystarczy teraz tylko pokazać, że taka zachłanna strategia dla tego rodzaju grafu konstruuje pokrycie cyklowe o maksymalnej wadze, co będzie oznaczało, że \textbf{MGREEDY} jest niegorszy niż \textbf{CONCATCYCLES}, czyli aproksymuje ze współczynnikiem 4. To też pokazali w swojej oryginalnej pracy Blum et al.

\paragraph{Algorytm \textit{TGREEDY}}

Algorytm \textbf{TGREEDY} najpierw wykonuje kroki od 1 do 2 z algorytmu \textbf{MGREEDY}, a następnie na uzyskanym zbiorze $T$, który odpowiada
maksymalnemu pokryciu cyklowemu grafu $G'_S$, wykonuje algorytm \textbf{GREEDY}. Aby pokazać, że \textbf{TGREEDY} ma pożądany współczynnik aproksymacj potrzebujemy wprowadzić parę dodatkowych definicji i lematów.

\begin{definition}{}{} Niech $\equiv$ będzie relacją równoważności dla słów, taką, że $s \equiv t \Leftrightarrow \exists u,v: s = uv, t = vu$. 
Wtedy, jeżeli $c$ to cykl na wierzchołkach (słowach) $i_0, ..., i_r$ to 
$strings(c)$ jest klasą równoważności $[pref(i_0, i_1)pref(i_1,i_2)...pref(i_{r-1},i_0)]_\equiv$.\\
Ponadto, $strings(c, i_k)$ zdefiniujemy jako
$pref(i_k, i_{k+1})...pref(i_{k-1},i_{k})$, gdzie indeksowanie jest modulo $r$. Oczywiście, $strings(c, i_k) \in strings(c)$.
\end{definition}
\begin{definition}{}{}
Niech $w(c)$, będzie wagą cyklu, czyli $w(c) = |s|, s\in strings(c)$. Dla wygody będziemy pisać, że $s\in c$ zamiast $s\in strings(c)$.
\end{definition}

\begin{lemma}{}{}\label{lemma9}
  Niech $c_1,c_2$ będą dwoma cyklami w minimalnym pokryciu cyklowym $C$, z $s_1\in c_1$ oraz $s_2 \in c_2$ Wtedy, $ov(s_1,s_2) \leq w(c_1) + w(c_2)$. 
\end{lemma}
\begin{lemma}{}{}\label{claim4}
Nadsłowo $\langle s_{i_0},...,s_{i_{r-1}} \rangle $ jest podsłowem $strings(c, i_0)s_{i_0}$.
\end{lemma}

\begin{theorem}{}{}
\textbf{TGREEDY} dla zbioru $S = \{s_1, ..., s_m \}$ zwraca nadsłowo $s$ nad $S$ o długości co najwyżej $3\cdot OPT(S)$.
\end{theorem}
\begin{proof}
Niech $n:=OPT(S)$, pokażemy, że $|s| \leq 3n$. Niech $T$ będzie zbiorem samo-nachodzących się słów uzyskanych przez \textbf{MGREEDY}
na $S$, a $C$ będzie minimalnym pokryciem cyklowym skonstruowanym przez \textbf{MGREEDY}. Dla każdego $x\in T$, niech $c_x$ oznacza cykl w $C$ odpowiadający 
słowie $x$ oraz niech $w_x = w(c_x)$ będzie jego wagą. Dla każdego zbioru słów $R$ niech $||R|| = \sum_{x\in R}|x|$ będzie 
sumaryczną długością wszystkich słów w zbiorze $R$. Niech $w = \sum_{x\in T} w_x$. Skoro, 
$CYC(G_S) \leq TSP(G_S) \leq OPT(S)$, wiemy że $w \leq n$.

Dzięki lematowi \cref{lemma9}, wiemy, że kompresja uzyskana w najkrótszym nadsłowie nad $T$ jest mniejsza niż $2w$, 
i.e $||T|| - n_T \leq 2w$, gdzie $n_T := OPT(T)$. Ponadto, znanym rezultatem jest (Tarhio i Ukkonen, 1988), że 
algorytm zachłanny osiąga kompresję ze współczynnikiem 2. Łącząc te dwa wyniki dostajemy, że:
$||T|| - |s| \geq (||T|| - n_T)/2 = ||T|| - n_T - (||T||-n_T)/2 \geq ||T|| - n_T - w$.

Z tego wynika, że $|s| \leq n_T + w$.

Dla każdego $x\in T$, niech $s_{i_x}$ będzie słowem z cyklu $c_x$, który jest prefiksem $x$. 
Niech $S' = \{s_{i_x} | x\in T \}, n' = OPT(S'), S'' = \{ strings(c_x, i_x)s_{i_x}|x\in T\}, n'' = OPT(S'')$.

Z lematu \cref{claim4} wynika, że nadsłowo $S''$ jest także nadsłowem $T$, więc $n_T \leq n''$. Dla dowolnej permutacji 
$\pi$ prawdą jest, że $|S''_\pi| \leq |S'_\pi| + \sum_{x\in T}w_x$, więc $n'' \leq n' + w$, gdzie 
$S'_\pi, S''_\pi$ jest nadsłowem uzyskanym poprzez połączenie słów z $S', S''$ w kolejności narzuconej przez $\pi$.

Wiemy, że $S' \subset S''$, a więc $n' \leq n$. Możemy teraz połączyć te wyniki i dostać, że
\[n_T \leq n'' \leq n' + w \leq n + w \]
Łącząc to z faktem, że $|s| \leq n_T + w$ dostajemy, że $|s| \leq n + 2w \leq 3n$.
\end{proof}

\paragraph{Uwagi końcowe}

Blum et al. pokazali dodatkowo, że \textbf{GREEDY} osiąga współczynnik aproksymacji równy 4, ale hipoteza czy ten współczynnik można obniżyć do 2 jest wciąż otwarta. 
Możnaby się pokusić wysunąć hipotezę, że \textbf{TGREEDY} jest lepszy niż \textbf{GREEDY}. Tak nie jest. Są przykłady zbioru $S$ dla których zarówno jeden jak i drugi wypada lepiej. 

Dla zbioru $S = \{c(ab)^k, (ab)^{k+1},(ba)^kc\}$ \textbf{TGREEDY} wygeneruje słowo $c(ab)^{k}ac(ab)^{k+1}a$ o długości $n=4k+6$, podczas gdy \textbf{GREEDY} zwróci słowo optymalne
$c(ab)^{k+1}ac$ o długośći $n=2k+5$. 

Z kolei dla zbioru $S = \{cab^k, ab^kab^ka, b^kdab^{k-1} \}$ \textbf{TGREEDY} wygeneruje słowo $cab^kdab^kab^ka$ o długości $3k+6$, podczas gdy 
\textbf{GREEDY} wygeneruje słowo $cab^kab^kab^kdab^{k-1}$ długości $4k+5$.

\subsection{Inne wyniki}

Oprócz algorytmu zachłannego opracowywano inne algorytmy o współczynnikach aproksymacji kolejno $\frac{8}{3}$ \citep{armen19962}, $\frac{5}{2}$ \citep{kaplan2005approximation, sweedyk2000boldmath}, $\frac{57}{23}$ \citep{mucha2013lyndon} i -- obecnie najlepszego -- $\frac{71}{30}$ \citep{paluch2014better}.
Większość z tych rezultatów polega na odpowiednio sprytnej redukcji do odpowiednio efektywnego algorytmu dla problemu maksymalnej ścieżki komiwojażera w grafie asymetrycznym.

Dodatkowo, jeśli przyjmiemy, że $|t_i| = r \ge 3$, to istnieje algorytm $\frac{r^2 + r - 4}{4r - 6}$-przybliżony \citep{golovnev2013approximating}. W \citet{braquelaire2018improving} ten wynik został nieznacznie poprawiony -- pozostaje on nadal jednak lepszy niż algorytm ogólny tylko dla $2 \le r \le 7$.
